namespace Skew {
  enum PassKind {
    LEXING
  }

  class LexingPass : Pass {
    const _cache LexingCache

    over kind PassKind {
      return .LEXING
    }

    over run(context PassContext) {
      if _cache != null {
        for source in context.inputs {
          context.tokens.append(_cache.tokenize(context.log, source))
        }
      } else {
        for source in context.inputs {
          context.tokens.append(tokenize(context.log, source))
        }
      }
    }
  }

  class LexingCache {
    var _sourceCache = StringMap<Source>.new
    var _tokenCache = StringMap<Entry>.new

    # Deduplicate sources so object identity doesn't change if the source doesn't change
    def source(name string, contents string) Source {
      var source = _sourceCache.get(name, null)
      if source == null || source.contents != contents {
        source = Source.new(name, contents)
        _sourceCache[name] = source
      }
      return source
    }

    def tokenize(log Log, source Source) List<Token> {
      var entry = _tokenCache.get(source.name, null)

      if entry == null || entry.source != source {
        const tempLog = Log.new
        const tokens = Skew.tokenize(tempLog, source)
        entry = Entry.new(source, tempLog, tokens)
        _tokenCache[source.name] = entry
      }

      log.diagnostics.append(entry.log.diagnostics)
      return entry.tokens
    }
  }

  namespace LexingCache {
    class Entry {
      const source Source
      const log Log
      const tokens List<Token>
    }
  }

  class Comment {
    var range Range
    var lines List<string>
    var hasGapBelow bool
    var isTrailing bool
  }

  namespace Comment {
    def concat(left List<Comment>, right List<Comment>) List<Comment> {
      if left == null { return right }
      if right == null { return left }
      const clone = left.clone
      clone.append(right)
      return clone
    }

    def lastTrailingComment(comments List<Comment>) Comment {
      if comments != null {
        var last = comments.last
        if last.isTrailing && last.lines.count == 1 {
          return last
        }
      }
      return null
    }

    def withoutLastTrailingComment(comments List<Comment>) List<Comment> {
      if comments != null {
        var last = comments.last
        if last.isTrailing && last.lines.count == 1 {
          return comments.slice(0, comments.count - 1)
        }
      }
      return comments
    }

    def firstTrailingComment(comments List<Comment>) Comment {
      if comments != null {
        var first = comments.first
        if first.isTrailing && first.lines.count == 1 {
          return first
        }
      }
      return null
    }

    def withoutFirstTrailingComment(comments List<Comment>) List<Comment> {
      if comments != null {
        var first = comments.first
        if first.isTrailing && first.lines.count == 1 {
          return comments.slice(1)
        }
      }
      return comments
    }
  }

  class Token {
    const range Range
    const kind TokenKind
    const comments List<Comment>
  }

  const REMOVE_WHITESPACE_BEFORE = {
    TokenKind.COLON: 0,
    TokenKind.COMMA: 0,
    TokenKind.DOT: 0,
    TokenKind.QUESTION_MARK: 0,
    TokenKind.RIGHT_PARENTHESIS: 0,
  }

  const FORBID_XML_AFTER = {
    TokenKind.CHARACTER: 0,
    TokenKind.DECREMENT: 0,
    TokenKind.DOUBLE: 0,
    TokenKind.DYNAMIC: 0,
    TokenKind.STRING_INTERPOLATION_END: 0,
    TokenKind.FALSE: 0,
    TokenKind.IDENTIFIER: 0,
    TokenKind.INCREMENT: 0,
    TokenKind.INT: 0,
    TokenKind.INT_BINARY: 0,
    TokenKind.INT_HEX: 0,
    TokenKind.INT_OCTAL: 0,
    TokenKind.NULL: 0,
    TokenKind.RIGHT_BRACE: 0,
    TokenKind.RIGHT_BRACKET: 0,
    TokenKind.RIGHT_PARENTHESIS: 0,
    TokenKind.STRING: 0,
    TokenKind.SUPER: 0,
    TokenKind.TRUE: 0,
  }

  # This is the inner loop from "flex", an ancient lexer generator. The output
  # of flex is pretty bad (obfuscated variable names and the opposite of modular
  # code) but it's fast and somewhat standard for compiler design. The code below
  # replaces a simple hand-coded lexer and offers much better performance.
  def tokenize(log Log, source Source) List<Token> {
    var comments List<Comment> = null
    var tokens List<Token> = []
    var text = source.contents
    var count = text.count
    var previousKind TokenKind = .NULL
    var previousWasComment = false
    var stack List<int> = []

    # For backing up
    var yy_last_accepting_state = 0
    var yy_last_accepting_cpos = 0

    # The current character pointer
    var yy_cp = 0

    while yy_cp < count {
      var yy_current_state = 1 # Reset the NFA
      var yy_bp = yy_cp # The pointer to the beginning of the token
      var yy_act TokenKind = .ERROR

      # Special-case string interpolation
      var c = text[yy_cp]
      var isStringInterpolation = c == '"'
      if c == ')' {
        for i = stack.count - 1; i >= 0; i-- {
          var kind = tokens[stack[i]].kind
          if kind == .STRING_INTERPOLATION_START {
            isStringInterpolation = true
          } else if kind != .LESS_THAN {
            break
          }
        }
      }
      if isStringInterpolation {
        var isExit = c == ')'
        yy_cp++
        while yy_cp < count {
          c = text[yy_cp++]
          if c == '"' {
            yy_act = isExit ? .STRING_INTERPOLATION_END : .STRING
            break
          }
          if c == '\\' {
            if yy_cp == count {
              break
            }
            c = text[yy_cp++]
            if c == '(' {
              yy_act = isExit ? .STRING_INTERPOLATION_CONTINUE : .STRING_INTERPOLATION_START
              break
            }
          }
        }
      }

      # Special-case XML literals
      else if c == '>' && !stack.isEmpty && tokens[stack.last].kind == .XML_START {
        yy_cp++
        yy_act = .XML_END
      }

      # Search for a match
      else {
        while yy_current_state != YY_JAM_STATE {
          if yy_cp >= count {
            break # This prevents syntax errors from causing infinite loops
          }
          c = text[yy_cp]
          var index = c < 127 ? c : 127 # All of the interesting characters are ASCII
          var yy_c = yy_ec[index]
          if yy_accept[yy_current_state] != .YY_INVALID_ACTION {
            yy_last_accepting_state = yy_current_state
            yy_last_accepting_cpos = yy_cp
          }
          while yy_chk[yy_base[yy_current_state] + yy_c] != yy_current_state {
            yy_current_state = yy_def[yy_current_state]
            if yy_current_state >= YY_ACCEPT_LENGTH {
              yy_c = yy_meta[yy_c]
            }
          }
          yy_current_state = yy_nxt[yy_base[yy_current_state] + yy_c]
          yy_cp++
        }

        # Find the action
        yy_act = yy_accept[yy_current_state]
        while yy_act == .YY_INVALID_ACTION {
          # Have to back up
          yy_cp = yy_last_accepting_cpos
          yy_current_state = yy_last_accepting_state
          yy_act = yy_accept[yy_current_state]
        }

        # Ignore whitespace
        if yy_act == .WHITESPACE {
          continue
        }

        # Stop at the end of the file
        if yy_act == .END_OF_FILE {
          break
        }
      }

      # Special-case XML literals
      if yy_act == .LESS_THAN && !(previousKind in FORBID_XML_AFTER) {
        yy_act = .XML_START
      }

      # This is the default action in flex, which is usually called ECHO
      else if yy_act == .ERROR {
        var iterator = Unicode.StringIterator.INSTANCE.reset(text, yy_bp)
        iterator.nextCodePoint
        var range = Range.new(source, yy_bp, iterator.index)
        log.syntaxErrorExtraData(range, range.toString)
        break
      }

      var tokenRange = Range.new(source, yy_bp, yy_cp)
      var tokenKind = yy_act
      var tokenComments List<Comment>

      # Have a nice error message for certain tokens
      if yy_act == .COMMENT_ERROR {
        log.syntaxErrorSlashComment(tokenRange)
        tokenKind = .COMMENT
      } else if yy_act == .NOT_EQUAL_ERROR {
        log.syntaxErrorOperatorTypo(tokenRange, "!=")
        tokenKind = .NOT_EQUAL
      } else if yy_act == .EQUAL_ERROR {
        log.syntaxErrorOperatorTypo(tokenRange, "==")
        tokenKind = .EQUAL
      }

      # Tokens that start with a greater than may need to be split, potentially multiple times
      var loop = true
      while loop {
        const tokenStartsWithGreaterThan = text[tokenRange.start] == '>'
        loop = false

        # Remove tokens from the stack if they aren't working out
        while !stack.isEmpty {
          const top = stack.last
          const topKind = tokens[top].kind

          # Stop parsing a type if we find a token that no type expression uses
          if topKind == .LESS_THAN && tokenKind != .LESS_THAN && tokenKind != .IDENTIFIER && tokenKind != .COMMA && tokenKind != .DYNAMIC &&
              tokenKind != .DOT && tokenKind != .LEFT_PARENTHESIS && tokenKind != .RIGHT_PARENTHESIS && !tokenStartsWithGreaterThan {
            stack.removeLast
          } else {
            break
          }
        }

        # Group open
        if tokenKind == .LEFT_PARENTHESIS || tokenKind == .LEFT_BRACE || tokenKind == .LEFT_BRACKET ||
            tokenKind == .LESS_THAN || tokenKind == .STRING_INTERPOLATION_START || tokenKind == .XML_START {
          stack.append(tokens.count)
        }

        # Group close
        else if tokenKind == .RIGHT_PARENTHESIS || tokenKind == .RIGHT_BRACE || tokenKind == .RIGHT_BRACKET ||
            tokenKind == .STRING_INTERPOLATION_END || tokenKind == .XML_END || tokenStartsWithGreaterThan {

          # Search for a matching opposite token
          while !stack.isEmpty {
            const top = stack.last
            const topKind = tokens[top].kind

            # Don't match ">" that don't work since they are just operators
            if tokenStartsWithGreaterThan && topKind != .LESS_THAN {
              break
            }

            # Consume the current token
            stack.removeLast

            # Stop if it's a match
            if tokenKind == .RIGHT_PARENTHESIS && topKind == .LEFT_PARENTHESIS ||
                tokenKind == .RIGHT_BRACKET && topKind == .LEFT_BRACKET ||
                tokenKind == .RIGHT_BRACE && topKind == .LEFT_BRACE ||
                tokenKind == .STRING_INTERPOLATION_END && topKind == .STRING_INTERPOLATION_START {
              break
            }

            # Special-case angle brackets matches and ignore tentative matches that didn't work out
            if topKind == .LESS_THAN && tokenStartsWithGreaterThan {

              # Break apart operators that start with a closing angle bracket
              if tokenKind != .GREATER_THAN {
                var start = tokenRange.start
                tokens.append(Token.new(Range.new(source, start, start + 1), .PARAMETER_LIST_END, null))
                tokenRange = Range.new(source, start + 1, tokenRange.end)
                tokenKind =
                  tokenKind == .SHIFT_RIGHT ? .GREATER_THAN :
                  tokenKind == .UNSIGNED_SHIFT_RIGHT ? .SHIFT_RIGHT :
                  tokenKind == .GREATER_THAN_OR_EQUAL ? .ASSIGN :
                  tokenKind == .ASSIGN_SHIFT_RIGHT ? .GREATER_THAN_OR_EQUAL :
                  tokenKind == .ASSIGN_UNSIGNED_SHIFT_RIGHT ? .ASSIGN_SHIFT_RIGHT :
                  .NULL
                assert(tokenKind != .NULL)
                loop = tokenKind != .GREATER_THAN_OR_EQUAL # Split this token again
              } else {
                tokenKind = .PARAMETER_LIST_END
              }

              # Convert the "<" into a bound for type parameter lists
              const topToken = tokens[top]
              tokens[top] = Token.new(topToken.range, .PARAMETER_LIST_START, topToken.comments)

              # Stop the search since we found a match
              break
            }
          }
        }
      }

      # Remove newlines based on the previous token to enable line continuations.
      # Make sure to be conservative. We want to be like Python, not like
      # JavaScript ASI! Anything that is at all ambiguous should be disallowed.
      #
      # Examples:
      # - "var x = 0 \n .toString"
      # - "var x = 0 # comment \n .toString"
      # - "var x = 0 \n # comment \n .toString"
      # - "var x = 0 \n ### \n multi-line comment \n ### \n return 0"
      #
      if previousKind == .NEWLINE && tokenKind == .NEWLINE {
        if comments != null && !previousWasComment {
          comments.last.hasGapBelow = true
        }
        previousWasComment = false
        continue
      } else if previousKind == .NEWLINE && tokenKind in REMOVE_WHITESPACE_BEFORE {
        var last = tokens.takeLast
        if last.comments != null {
          comments ?= []
          comments.append(last.comments)
        }
      }

      # Attach comments to tokens instead of having comments be tokens
      previousWasComment = tokenKind == .COMMENT
      if previousWasComment {
        comments ?= []
        if comments.isEmpty || comments.last.hasGapBelow {
          comments.append(Comment.new(tokenRange, [], false, false))
        }
        var line = source.contents.slice(tokenRange.start + 1, tokenRange.end)
        var hashes = 0
        for j in 0..line.count {
          if line[j] != '#' {
            break
          }
          hashes++
        }
        if hashes != 0 {
          line = "/".repeat(hashes) + line.slice(hashes)
        }
        comments.last.lines.append(line)
        continue
      }

      previousKind = tokenKind

      if previousKind != .NEWLINE {
        tokenComments = comments
        comments = null
      }

      # Capture trailing comments
      if !tokens.isEmpty && comments != null && comments.count == 1 &&
          comments.first.lines.count == 1 && !comments.first.hasGapBelow {
        comments.first.isTrailing = true
        tokenComments = comments
        comments = null
      }

      # Accumulate the token for this iteration
      tokens.append(Token.new(tokenRange, tokenKind, tokenComments))
    }

    # Every token stream ends in END_OF_FILE
    tokens.append(Token.new(Range.new(source, yy_cp, yy_cp), .END_OF_FILE, comments))

    # Also return preprocessor token presence so the preprocessor can be avoided
    return tokens
  }

  enum TokenKind {
    # Type parameters are surrounded by "<" and ">"
    PARAMETER_LIST_END
    PARAMETER_LIST_START

    # XML entities are surrounded by "<" and ">" (or "</" and "/>" but those are defined by flex)
    XML_END
    XML_START

    # String interpolation looks like "start\( 1 )continue( 2 )end"
    STRING_INTERPOLATION_CONTINUE
    STRING_INTERPOLATION_END
    STRING_INTERPOLATION_START

    def toString string {
      assert(self in _toString)
      return _toString[self]
    }
  }

  namespace TokenKind {
    const _toString = {
      COMMENT: "comment",
      NEWLINE: "newline",
      WHITESPACE: "whitespace",

      AS: "\"as\"",
      BREAK: "\"break\"",
      CASE: "\"case\"",
      CATCH: "\"catch\"",
      CONST: "\"const\"",
      CONTINUE: "\"continue\"",
      DEFAULT: "\"default\"",
      DYNAMIC: "\"dynamic\"",
      ELSE: "\"else\"",
      FALSE: "\"false\"",
      FINALLY: "\"finally\"",
      FOR: "\"for\"",
      IF: "\"if\"",
      IN: "\"in\"",
      IS: "\"is\"",
      NULL: "\"null\"",
      RETURN: "\"return\"",
      SUPER: "\"super\"",
      SWITCH: "\"switch\"",
      THROW: "\"throw\"",
      TRUE: "\"true\"",
      TRY: "\"try\"",
      VAR: "\"var\"",
      WHILE: "\"while\"",

      ARROW: "\"=>\"",
      ASSIGN: "\"=\"",
      ASSIGN_BITWISE_AND: "\"&=\"",
      ASSIGN_BITWISE_OR: "\"|=\"",
      ASSIGN_BITWISE_XOR: "\"^=\"",
      ASSIGN_DIVIDE: "\"/=\"",
      ASSIGN_INDEX: "\"[]=\"",
      ASSIGN_MINUS: "\"-=\"",
      ASSIGN_MODULUS: "\"%%=\"",
      ASSIGN_MULTIPLY: "\"*=\"",
      ASSIGN_PLUS: "\"+=\"",
      ASSIGN_POWER: "\"**=\"",
      ASSIGN_REMAINDER: "\"%=\"",
      ASSIGN_SHIFT_LEFT: "\"<<=\"",
      ASSIGN_SHIFT_RIGHT: "\">>=\"",
      ASSIGN_UNSIGNED_SHIFT_RIGHT: "\">>>=\"",
      BITWISE_AND: "\"&\"",
      BITWISE_OR: "\"|\"",
      BITWISE_XOR: "\"^\"",
      COLON: "\":\"",
      COMMA: "\",\"",
      COMPARE: "\"<=>\"",
      DECREMENT: "\"--\"",
      DIVIDE: "\"/\"",
      DOT: "\".\"",
      DOT_DOT: "\"..\"",
      DOUBLE_COLON: "\"::\"",
      EQUAL: "\"==\"",
      GREATER_THAN: "\">\"",
      GREATER_THAN_OR_EQUAL: "\">=\"",
      INCREMENT: "\"++\"",
      INDEX: "\"[]\"",
      LEFT_BRACE: "\"{\"",
      LEFT_BRACKET: "\"[\"",
      LEFT_PARENTHESIS: "\"(\"",
      LESS_THAN: "\"<\"",
      LESS_THAN_OR_EQUAL: "\"<=\"",
      LIST: "\"[...]\"",
      LIST_NEW: "\"[new]\"",
      LOGICAL_AND: "\"&&\"",
      LOGICAL_OR: "\"||\"",
      MINUS: "\"-\"",
      MODULUS: "\"%%\"",
      MULTIPLY: "\"*\"",
      NOT: "\"!\"",
      NOT_EQUAL: "\"!=\"",
      NULL_DOT: "\"?.\"",
      NULL_JOIN: "\"??\"",
      PLUS: "\"+\"",
      POWER: "\"**\"",
      QUESTION_MARK: "\"?\"",
      REMAINDER: "\"%\"",
      RIGHT_BRACE: "\"}\"",
      RIGHT_BRACKET: "\"]\"",
      RIGHT_PARENTHESIS: "\")\"",
      SEMICOLON: "\";\"",
      SET: "\"{...}\"",
      SET_NEW: "\"{new}\"",
      SHIFT_LEFT: "\"<<\"",
      SHIFT_RIGHT: "\">>\"",
      TILDE: "\"~\"",
      UNSIGNED_SHIFT_RIGHT: "\">>>\"",

      ANNOTATION: "annotation",
      CHARACTER: "character",
      DOUBLE: "double",
      END_OF_FILE: "end of input",
      IDENTIFIER: "identifier",
      INT: "integer",
      INT_BINARY: "integer",
      INT_HEX: "integer",
      INT_OCTAL: "integer",
      STRING: "string",

      PARAMETER_LIST_END: "\">\"",
      PARAMETER_LIST_START: "\"<\"",

      XML_CHILD: "\"<>...</>\"",
      XML_END: "\">\"",
      XML_END_EMPTY: "\"/>\"",
      XML_START: "\"<\"",
      XML_START_CLOSE: "\"</\"",

      STRING_INTERPOLATION_CONTINUE: "string interpolation",
      STRING_INTERPOLATION_END: "string interpolation",
      STRING_INTERPOLATION_START: "string interpolation",
    }
  }
}
